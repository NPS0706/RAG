# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UnNhR720X1mefHHsxG0ZpO7Qs_w7BoFV
"""

!pip install -q langchain faiss-cpu sentence-transformers groq python-dotenv

import os
os.environ["GROQ_API_KEY"] = "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"

from google.colab import files
uploaded = files.upload()

!pip install -q pymupdf

import fitz  # PyMuPDF

# Get filename
filename = list(uploaded.keys())[0]

# Extract text from all pages
doc = fitz.open(filename)
text = ""
for page in doc:
    text += page.get_text()

print(f"âœ… Extracted {len(text)} characters from PDF.")

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Use RecursiveCharacterTextSplitter to chunk the raw text
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,     # Max size per chunk (in characters)
    chunk_overlap=50,   # Overlap to retain context
    separators=["\n\n", "\n", ".", " ", ""]
)

# Split into documents
documents = text_splitter.create_documents([text])

# Preview how many chunks created
print(f"âœ… Total chunks created: {len(documents)}")
print("\nðŸ”¹ Sample chunk:\n")
print(documents[0].page_content[:1000])

!pip install -q sentence-transformers faiss-cpu langchain

!pip install -U langchain-community

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Load the embedding model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create FAISS vector store from documents
vectorstore = FAISS.from_documents(documents, embedding_model)

# Save FAISS index (optional, for later reuse)
vectorstore.save_local("faiss_icd10_index")

print("âœ… FAISS vector store created with embedded document chunks.")

!pip install groq

from groq import Groq

groq_api_key = "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"
client = Groq(api_key=groq_api_key)

def rag_qa(query, k=5):
    # 1. Retrieve top-k relevant chunks from FAISS
    docs = vectorstore.similarity_search(query, k=k)
    context = "\n\n".join([doc.page_content for doc in docs])

    # 2. Build the prompt
    final_prompt = f"""You are a helpful medical assistant.
Use the following context from the ICD-10 to answer the question clearly.

Context:
{context}

Question: {query}

Answer:"""

    # 3. Query Groq LLM
    response = client.chat.completions.create(
        model="llama3-8b-8192",  # âœ… Use supported model
        messages=[{"role": "user", "content": final_prompt}],
        temperature=0.2,
        max_tokens=512
    )

    return response.choices[0].message.content.strip()

response = rag_qa("What are the diagnostic criteria for schizophrenia?")
print("ðŸ”¹Answer:\n", response)

response = rag_qa("Give me the correct coded classification for the following diagnosis: Recurrent depressive disorder, currently in remission")
print("ðŸ”¹Answer:\n", response)
